% Generated by Paperpile. Check out http://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@INCOLLECTION{Joachims1998-zz,
  title     = "Text categorization with Support Vector Machines: Learning with
               many relevant features",
  booktitle = "Machine Learning: {ECML-98}",
  author    = "Joachims, Thorsten",
  abstract  = "This paper explores the use of Support Vector Machines (SVMs)
               for learning text classifers from examples. It analyzes the
               particular properties of learning with text data and
               identies,why SVMs are appropriate for this task. Empirical
               results support the theoretical findings. SVMs achieve
               substantial improvements over the currently best performing
               methods and they behave robustly over a variety of diferent
               learning tasks. Furthermore, they are fully automatic,
               eliminating the need for manual parameter tuning.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "137--142",
  series    = "Lecture Notes in Computer Science",
  month     =  "21~" # apr,
  year      =  1998,
  keywords  = "Multi Label Classification"
}

@ARTICLE{Borchani2016-xi,
  title     = "Mining multi-dimensional concept-drifting data streams using
               Bayesian network classifiers",
  author    = "Borchani, Hanen and Larra\~{n}aga, Pedro and Gama, Jo\~{a}o and
               Bielza, Concha",
  journal   = "Intelligent Data Analysis",
  publisher = "IOS Press",
  volume    =  20,
  number    =  2,
  pages     = "257--280",
  year      =  2016,
  keywords  = "Multi Label Classification"
}

@INCOLLECTION{Puchala2016-uh,
  title     = "Comparison of Multi-label and Multi-perspective Classifiers in
               Multi-task Pattern Recognition Problems",
  booktitle = "Proceedings of the 9th International Conference on Computer
               Recognition Systems {CORES} 2015",
  author    = "Pucha\l{}a, Edward and Reisner, Krzysztof",
  publisher = "Springer International Publishing",
  pages     = "275--282",
  series    = "Advances in Intelligent Systems and Computing",
  year      =  2016,
  keywords  = "Multi Label Classification"
}

@ARTICLE{Ecmlpkdd_undated-yr,
  title    = "{LEARNING} {FROM} {MULTI-LABEL} {DATA}",
  author   = "Ecmlpkdd, A T",
  keywords = "Multi Label Classification"
}

@ARTICLE{Bungum_undated-ue,
  title    = "{Self-Organizing} Maps for Classification of a {Multi-Labeled}
              Corpus",
  author   = "Bungum, Lars and Gamb{\"{a}}ck, Bj{\"{o}}rn",
  keywords = "Multi Label Classification"
}

@ARTICLE{Teisseyre2016-xf,
  title         = "Asymptotic consistency and order specification for logistic
                   classifier chains in multi-label learning",
  author        = "Teisseyre, Pawe\l",
  abstract      = "Classifier chains are popular and effective method to tackle
                   a multi-label classification problem. The aim of this paper
                   is to study the asymptotic properties of the chain model in
                   which the conditional probabilities are of the logistic
                   form. In particular we find conditions on the number of
                   labels and the distribution of feature vector under which
                   the estimated mode of the joint distribution of labels
                   converges to the true mode. Best of our knowledge, this
                   important issue has not yet been studied in the context of
                   multi-label learning. We also investigate how the order of
                   model building in a chain influences the estimation of the
                   joint distribution of labels. We establish the link between
                   the problem of incorrect ordering in the chain and incorrect
                   model specification. We propose a procedure of determining
                   the optimal ordering of labels in the chain, which is based
                   on using measures of correct specification and allows to
                   find the ordering such that the consecutive logistic models
                   are best possibly specified. The other important question
                   raised in this paper is how accurately can we estimate the
                   joint posterior probability when the ordering of labels is
                   wrong or the logistic models in the chain are incorrectly
                   specified. The numerical experiments illustrate the
                   theoretical results.",
  month         =  "24~" # feb,
  year          =  2016,
  keywords      = "Multi Label Classification",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.07466"
}

@ARTICLE{Teisseyre2016-kt,
  title         = "Feature ranking for multi-label classification using Markov
                   Networks",
  author        = "Teisseyre, Pawe\l",
  abstract      = "We propose a simple and efficient method for ranking
                   features in multi-label classification. The method produces
                   a ranking of features showing their relevance in predicting
                   labels, which in turn allows to choose a final subset of
                   features. The procedure is based on Markov Networks and
                   allows to model the dependencies between labels and features
                   in a direct way. In the first step we build a simple network
                   using only labels and then we test how much adding a single
                   feature affects the initial network. More specifically, in
                   the first step we use the Ising model whereas the second
                   step is based on the score statistic, which allows to test a
                   significance of added features very quickly. The proposed
                   approach does not require transformation of label space,
                   gives interpretable results and allows for attractive
                   visualization of dependency structure. We give a theoretical
                   justification of the procedure by discussing some
                   theoretical properties of the Ising model and the score
                   statistic. We also discuss feature ranking procedure based
                   on fitting Ising model using $l_1$ regularized logistic
                   regressions. Numerical experiments show that the proposed
                   methods outperform the conventional approaches on the
                   considered artificial and real datasets.",
  month         =  "24~" # feb,
  year          =  2016,
  keywords      = "Multi Label Classification",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.07464"
}

@ARTICLE{Chen2009-jy,
  title    = "Constructing a decision tree from data with hierarchical class
              labels",
  author   = "Chen, Yen-Liang and Hu, Hsiao-Wei and Tang, Kwei",
  abstract = "Most decision tree classifiers are designed to classify the data
              with categorical or Boolean class labels. Unfortunately, many
              practical classification problems concern data with class labels
              that are naturally organized as a hierarchical structure, such as
              test scores. In the hierarchy, the ranges in the upper levels are
              less specific but easier to predict, while the ranges in the
              lower levels are more specific but harder to predict. To build a
              decision tree from this kind of data, we must consider how to
              classify data so that the class label can be as specific as
              possible while also ensuring the highest possible accuracy of the
              prediction. To the best of our knowledge, no previous research
              has considered the induction of decision trees from data with
              hierarchical class labels. This paper proposes a novel
              classification algorithm for learning decision tree classifiers
              from data with hierarchical class labels. Empirical results show
              that the proposed method is efficient and effective in both
              prediction accuracy and prediction specificity.",
  journal  = "Expert Syst. Appl.",
  volume   =  36,
  number   = "3, Part 1",
  pages    = "4838--4847",
  month    =  apr,
  year     =  2009,
  keywords = "Classification; Decision tree; Hierarchical class label;Multi
              Label Classification"
}

@ARTICLE{Chen2003-va,
  title    = "Constructing a multi-valued and multi-labeled decision tree",
  author   = "Chen, Yen-Liang and Hsu, Chang-Ling and Chou, Shih-Chieh",
  abstract = "Most decision tree classifiers are designed to classify the
              objects whose attributes and class labels are single values.
              However, many practical classification problems need to deal with
              multi-valued and multi-labeled data. For example, a customer data
              in a tour company may have multi-valued attributes such as the
              cars, the hobbies and the houses of the customer and multiple
              labels corresponding to the tours joined before. If the company
              intends to use customers' data to build a classifier to predict
              what kinds of customers are likely to participate in what kinds
              of tours; then a requirement arises immediately is how to design
              a new classification algorithm to classify the multi-valued and
              multi-labeled data. Therefore, this research has engaged in
              developing such a new classifier. We found that the design of
              some major functions used in our classifier is different from the
              existing ones, including how to select the next splitting
              attribute, when to stop the splitting of a node, how to determine
              a node's labels, and how to predict the labels of a new data. In
              this paper, all these issues are addressed and the problems are
              solved. The simulation result shows that the proposed algorithm
              performs well both in computing time and in accuracy.",
  journal  = "Expert Syst. Appl.",
  volume   =  25,
  number   =  2,
  pages    = "199--209",
  month    =  aug,
  year     =  2003,
  keywords = "Decision tree; Data mining; Classification; Multi-valued
              attribute; Multi-labeled attribute; Prediction; Customer relation
              management;Algorithm Adaptation;Multi Label Classification"
}

@INPROCEEDINGS{Veloso2007-el,
  title       = "Multi-label lazy associative classification",
  booktitle   = "{PKDD}",
  author      = "Veloso, Adriano and Meira, Jr, Wagner and Gon\c{c}alves,
                 Marcos Andr\'{e} and Zaki, Mohammed Javeed",
  abstract    = "Most current work on classification has been focused on
                 learning froma set of instances that are associated with a
                 single label (i.e., single-label classi-fication). However,
                 many applications, such as gene functional prediction and text
                 categorization, may allow the instances to be associated with
                 multiple labels simultaneously. Multi-label classification is
                 a generalization of single-label classification, and its
                 generality makes it much more difficult to solve. Despite its
                 importance, research on multi-label classification is still
                 lacking. Common approaches simply learn independent binary
                 classifiers for each label, and do not exploit dependencies
                 among labels. Also, several small disjuncts may appear due to
                 the possibly large number of label combinations, and
                 neglecting these small disjuncts may degrade classification
                 accuracy. In this paper we propose a multi-label lazy
                 associative classifier, which progressively exploits
                 dependencies among labels. Further, since in our lazy strategy
                 the classification model is induced on an instance-based
                 fashion, the proposed approach can provide a better coverage
                 of small disjuncts. Gains of up to 24\% are observed when the
                 proposed approach is compared against the state-of-the-art
                 multi-label classifiers.",
  volume      =  4702,
  pages       = "605--612",
  institution = "Springer",
  year        =  2007,
  keywords    = "Algorithm Adaptation;Multi Label Classification"
}

@INPROCEEDINGS{Thabtah2004-vz,
  title     = "{MMAC}: a new multi-class, multi-label associative
               classification approach",
  booktitle = "Data Mining, 2004. {ICDM} '04. Fourth {IEEE} International
               Conference on",
  author    = "Thabtah, F A and Cowling, P and Peng, Yonghong",
  abstract  = "Building fast and accurate classifiers for large-scale databases
               is an important task in data mining. There is growing evidence
               that integrating classification and association rule mining
               together can produce more efficient and accurate classifiers
               than traditional classification techniques. In this paper, the
               problem of producing rules with multiple labels is investigated.
               We propose a new associative classification approach called
               multi-class, multi-label associative classification (MMAC). This
               paper also presents three measures for evaluating the accuracy
               of data mining classification approaches to a wide range of
               traditional and multi-label classification problems. Results for
               28 different datasets show that the MMAC approach is an accurate
               and effective classification technique, highly competitive and
               scalable in comparison with other classification approaches.",
  pages     = "217--224",
  month     =  nov,
  year      =  2004,
  keywords  = "data mining;pattern classification;very large
               databases;MMAC;association rule mining;classification
               technique;data mining;large-scale databases;multiclass
               multilabel associative classification;multiple
               labels;Association rules;Data mining;Decision trees;Deductive
               databases;Intelligent structures;Large-scale systems;Processor
               scheduling;Testing;Text categorization;Training data;Algorithm
               Adaptation;Multi Label Classification"
}

@INCOLLECTION{Spyromitros2008-rs,
  title     = "An Empirical Study of Lazy Multilabel Classification Algorithms",
  booktitle = "Artificial Intelligence: Theories, Models and Applications",
  author    = "Spyromitros, E and Tsoumakas, G and Vlahavas, Ioannis",
  publisher = "Springer Berlin Heidelberg",
  pages     = "401--406",
  series    = "Lecture Notes in Computer Science",
  month     =  "2~" # oct,
  year      =  2008,
  keywords  = "Algorithm Adaptation;Multi Label Classification"
}

@ARTICLE{Crammer2003-su,
	title     = "A Family of Additive Online Algorithms for Category Ranking",
	author    = "Crammer, Koby and Singer, Yoram",
	journal   = "J. Mach. Learn. Res.",
	publisher = "JMLR.org",
	volume    =  3,
	pages     = "1025--1058",
	month     =  mar,
	year      =  2003
}

@ARTICLE{Yang1999-ah,
	title     = "An Evaluation of Statistical Approaches to Text Categorization",
	author    = "Yang, Yiming",
	journal   = "Inf. Retr. Boston.",
	publisher = "Kluwer Academic Publishers",
	volume    =  1,
	number    = "1-2",
	pages     = "69--90",
	year      =  1999
}

@ARTICLE{Lafferty2001-ov,
	title    = "Conditional random fields: Probabilistic models for segmenting
	and labeling sequence data",
	author   = "Lafferty, John and McCallum, Andrew and Pereira, Fernando C N",
	abstract = "We present conditional random fields, a framework for building
	probabilistic models to segment and label sequence data.
	Conditional random fields offer several advantages over hidden
	Markov models and stochastic grammars for such tasks, including
	the ability to relax strong independence assumptions made in
	those models. Conditional random fields also avoid a fundamental
	limitation of maximum entropy Markov models (MEMMs) and other
	discriminative Markov models based on directed graphical models,
	which can be biased towards states with few successor states. We
	present iterative parameter estimation algorithms for conditional
	random fields and compare the performance of the resulting models
	to HMMs and MEMMs on synthetic and natural-language data",
	journal  = "Proceedings of the Eighteenth International Conference on Machine
	Learning (ICML 2001)",
	pages    = "282--289",
	year     =  2001
}

@ARTICLE{Schapire2000-yt,
	title     = "{BoosTexter}: A Boosting-based System for Text Categorization",
	author    = "Schapire, Robert E and Singer, Yoram",
	journal   = "Mach. Learn.",
	publisher = "Kluwer Academic Publishers",
	volume    =  39,
	number    = "2-3",
	pages     = "135--168",
	year      =  2000
}

@ARTICLE{Furnkranz2008-rf,
	title     = "Multilabel classification via calibrated label ranking",
	author    = "F{\"{u}}rnkranz, Johannes and H{\"{u}}llermeier, Eyke and
	Menc\'{\i}a, Eneldo Loza and Brinker, Klaus",
	abstract  = "Abstract Label ranking studies the problem of learning a mapping
	from instances to rankings over a predefined set of labels.
	Hitherto existing approaches to label ranking implicitly operate
	on an underlying (utility) scale which is not calibrated in the
	sense that it lacks a ...",
	journal   = "Mach. Learn.",
	publisher = "Springer US",
	volume    =  73,
	number    =  2,
	pages     = "133--153",
	month     =  "6~" # aug,
	year      =  2008
}

@ARTICLE{Hullermeier2008-co,
	title     = "Label ranking by learning pairwise preferences",
	author    = "H{\"{u}}llermeier, Eyke and F{\"{u}}rnkranz, Johannes and Cheng,
	Weiwei and Brinker, Klaus",
	abstract  = "Preference learning is an emerging topic that appears in
	different guises in the recent literature. This work focuses on
	a particular learning scenario called label ranking, where the
	problem is to learn a mapping from instances to rankings over a
	finite number of labels. Our approach for learning such a
	mapping, called ranking by pairwise comparison (RPC), first
	induces a binary preference relation from suitable training data
	using a natural extension of pairwise classification. A ranking
	is then derived from the preference relation thus obtained by
	means of a ranking procedure, whereby different ranking methods
	can be used for minimizing different loss functions. In
	particular, we show that a simple (weighted) voting strategy
	minimizes risk with respect to the well-known Spearman rank
	correlation. We compare RPC to existing label ranking methods,
	which are based on scoring individual labels instead of
	comparing pairs of labels. Both empirically and theoretically,
	it is shown that RPC is superior in terms of computational
	efficiency, and at least competitive in terms of accuracy.",
	journal   = "Artif. Intell.",
	publisher = "Elsevier",
	volume    =  172,
	number    = "16--17",
	pages     = "1897--1916",
	month     =  nov,
	year      =  2008,
	keywords  = "Preference learning; Ranking; Pairwise classification;
	Constraint classification"
}

@MISC{Brinker_undated-zz,
  title        = "Case-based Multilabel Ranking",
  author       = "Brinker, K and H{\"{u}}llermeier, E",
  abstract     = "AAAI advances the understanding of the mechanisms underlying
                  thought and intelligent behavior and their embodiment in
                  machines.",
  howpublished = "\url{http://www.aaai.org/Library/IJCAI/2007/ijcai07-112.php}",
  note         = "Accessed: 2016-2-28",
  keywords     = "Algorithm Adaptation;Multi Label Classification"
}

@INCOLLECTION{Wieczorkowska2006-am,
  title     = "{Multi-Label} Classification of Emotions in Music",
  booktitle = "Intelligent Information Processing and Web Mining",
  author    = "Wieczorkowska, Alicja and Synak, Piotr and Ra\'{s}, Zbigniew W",
  publisher = "Springer Berlin Heidelberg",
  pages     = "307--315",
  series    = "Advances in Soft Computing",
  year      =  2006,
  keywords  = "Algorithm Adaptation;Multi Label Classification"
}

@INCOLLECTION{Charte2012-tw,
  title     = "Improving Multi-label Classifiers via Label Reduction with
               Association Rules",
  booktitle = "Hybrid Artificial Intelligent Systems",
  author    = "Charte, Francisco and Rivera, Antonio and del Jesus, Mar\'{\i}a
               Jos\'{e} and Herrera, Francisco",
  publisher = "Springer Berlin Heidelberg",
  pages     = "188--199",
  series    = "Lecture Notes in Computer Science",
  month     =  "28~" # mar,
  year      =  2012,
  keywords  = "Multi Label Classification"
}

@ARTICLE{Mao2013-gg,
  title    = "A {Multi-Label} Classification Using {KNN} and {FP-Growth}
              Techniques",
  author   = "Mao, Jia Li and Li, Ming Dong and Liu, Min",
  journal  = "AMRO",
  volume   = "791-793",
  pages    = "1554--1557",
  month    =  sep,
  year     =  2013,
  keywords = "Algorithm Adaptation;Multi Label Classification"
}

@INPROCEEDINGS{Zi-Jie_Chen_undated-mi,
  title           = "Latent semantic {KNN} algorithm for multi-label learning",
  booktitle       = "2014 International Conference on Machine Learning and
                     Cybernetics",
  author          = "{Zi-Jie Chen} and {Zhi-Feng Hao}",
  publisher       = "IEEE",
  pages           = "278--284",
  keywords        = "Multi Label Classification",
  conference      = "2014 International Conference on Machine Learning and
                     Cybernetics (ICMLC)"
}

@INCOLLECTION{Luo2005-ac,
  title     = "Evaluation of Two Systems on Multi-class Multi-label Document
               Classification",
  booktitle = "Foundations of Intelligent Systems",
  author    = "Luo, Xiao and Nur Zincir-Heywood, A",
  publisher = "Springer Berlin Heidelberg",
  pages     = "161--169",
  series    = "Lecture Notes in Computer Science",
  month     =  "25~" # may,
  year      =  2005,
  keywords  = "Algorithm Adaptation;Multi Label Classification"
}

@INCOLLECTION{Godbole2004-su,
  title     = "Discriminative Methods for Multi-labeled Classification",
  booktitle = "Advances in Knowledge Discovery and Data Mining",
  author    = "Godbole, Shantanu and Sarawagi, Sunita",
  publisher = "Springer Berlin Heidelberg",
  pages     = "22--30",
  series    = "Lecture Notes in Computer Science",
  month     =  "26~" # may,
  year      =  2004,
  keywords  = "Algorithm Adaptation;Binary Relevance;Multi Label Classification"
}

@INPROCEEDINGS{Ueda2002-gd,
  title     = "Parametric mixture models for multi-labeled text",
  booktitle = "Advances in neural information processing systems",
  author    = "Ueda, Naonori and Saito, Kazumi",
  abstract  = "We propose probabilistic generative models,called parametric
               mixture models (PMMs), for multiclass,multi-labeled text
               categorization problem. Conventionally, the binary
               classification approach has been employed,i n which whether or
               not text belongs to a category is judged by the binary
               classifier for every category. In contrast,our approach can
               simultaneously detect multiple categories of text using PMMs. We
               derive eficient learning and prediction algorithms for PMMs. We
               also empirically show that our method could significantly
               outperform the conventional binary methods when applied to
               multi-labeled text categorization using real World Wide Web
               pages.",
  pages     = "721--728",
  year      =  2002,
  keywords  = "Problem Transformation;Binary Relevance;Multi Label
               Classification"
}

@ARTICLE{Zhang2006-vf,
  title    = "Multilabel Neural Networks with Applications to Functional
              Genomics and Text Categorization",
  author   = "Zhang, Min-Ling and Zhou, Zhi-Hua",
  abstract = "In multilabel learning, each instance in the training set is
              associated with a set of labels and the task is to output a label
              set whose size is unknown a priori for each unseen instance. In
              this paper, this problem is addressed in the way that a neural
              network algorithm named BP-MLL, i.e., backpropagation for
              multilabel learning, is proposed. It is derived from the popular
              backpropagation algorithm through employing a novel error
              function capturing the characteristics of multilabel learning,
              i.e., the labels belonging to an instance should be ranked higher
              than those not belonging to that instance. Applications to two
              real-world multilabel learning problems, i.e., functional
              genomics and text categorization, show that the performance of
              BP-MLL is superior to that of some well-established multilabel
              learning algorithms",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  volume   =  18,
  number   =  10,
  pages    = "1338--1351",
  month    =  oct,
  year     =  2006,
  keywords = "backpropagation;biology computing;genetics;text
              analysis;BP-MLL;backpropagation algorithm;functional
              genomics;multilabel neural network learning;text
              categorization;Backpropagation
              algorithms;Biochemistry;Bioinformatics;Data
              mining;Genomics;Government;Layout;Learning systems;Neural
              networks;Text categorization;Machine
              learning;backpropagation;data mining;functional
              genomics;multilabel learning;neural networks;text
              categorization.;Perceptrons;Algorithm Adaptation;Multi Label
              Classification"
}

@INCOLLECTION{Streich2008-vu,
  title     = "Classification of Multi-labeled Data: A Generative Approach",
  booktitle = "Machine Learning and Knowledge Discovery in Databases",
  author    = "Streich, Andreas P and Buhmann, Joachim M",
  abstract  = "Abstract Multi-label classification assigns a data item to one
               or several classes. This problem of multiple labels arises in
               fields like acoustic and visual scene analysis, news reports and
               medical diagnosis. In a generative framework, data with multiple
               labels can be interpreted ...",
  publisher = "Springer Berlin Heidelberg",
  pages     = "390--405",
  series    = "Lecture Notes in Computer Science",
  month     =  "15~" # sep,
  year      =  2008,
  keywords  = "Algorithm Adaptation;Multi Label Classification"
}

@INPROCEEDINGS{Huang2012-dv,
  title       = "{Multi-Label} Learning by Exploiting Label Correlations
                 Locally",
  booktitle   = "{AAAI}",
  author      = "Huang, Sheng-Jun and Zhou, Zhi-Hua and Zhou, Z H",
  institution = "Citeseer",
  year        =  2012,
  keywords    = "Label Correlation;Multi Label Classification"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{McCallum1999-iz,
  title     = "Multi-label text classification with a mixture model trained by
               {EM}",
  booktitle = "{AAAIâ€™99} workshop on text learning",
  author    = "McCallum, Andrew",
  pages     = "1--7",
  year      =  1999,
  keywords  = "Influent;Multi Label Classification"
}

@INPROCEEDINGS{Schapire1998-fq,
  title     = "Improved boosting algorithms using confidence-rated predictions",
  booktitle = "Proceedings of the eleventh annual conference on Computational
               learning theory",
  author    = "Schapire, Robert E and Singer, Yoram",
  publisher = "ACM",
  pages     = "80--91",
  month     =  "24~" # jul,
  year      =  1998,
  keywords  = "Multi Label Classification"
}

@INCOLLECTION{De_Comite2003-lg,
  title     = "Learning Multi-label Alternating Decision Trees from Texts and
               Data",
  booktitle = "Machine Learning and Data Mining in Pattern Recognition",
  author    = "De Comit\'{e}, Francesco and Gilleron, R\'{e}mi and Tommasi,
               Marc",
  publisher = "Springer Berlin Heidelberg",
  pages     = "35--49",
  series    = "Lecture Notes in Computer Science",
  month     =  "5~" # jul,
  year      =  2003,
  keywords  = "Algorithm Adaptation;Multi Label Classification"
}

@INPROCEEDINGS{Trohidis2008-oa,
  title     = "{Multi-Label} Classification of Music into Emotions",
  booktitle = "{ISMIR}",
  author    = "Trohidis, Konstantinos and Tsoumakas, Grigorios and Kalliris,
               George and Vlahavas, Ioannis P",
  abstract  = "ABSTRACT In this paper, the automated detection of emotion in
               music is modeled as a multilabel classification task, where a
               piece of music may belong to more than one class. Four
               algorithms are evaluated and compared in this task. Furthermore,
               the predictive ...",
  publisher = "books.google.com",
  volume    =  8,
  pages     = "325--330",
  year      =  2008,
  keywords  = "Influent;Multi Label Classification"
}

@ARTICLE{Esuli2008-on,
  title     = "Boosting multi-label hierarchical text categorization",
  author    = "Esuli, Andrea and Fagni, Tiziano and Sebastiani, Fabrizio",
  abstract  = "Abstract Hierarchical Text Categorization (HTC) is the task of
               generating (usually by means of supervised learning algorithms)
               text classifiers that operate on hierarchically structured
               classification schemes. Notwithstanding the fact that most
               large-sized classification ...",
  journal   = "Inf. Retr. Boston.",
  publisher = "Springer Netherlands",
  volume    =  11,
  number    =  4,
  pages     = "287--313",
  month     =  "28~" # feb,
  year      =  2008,
  keywords  = "Label Correlation;Algorithm Adaptation;Multi Label
               Classification"
}

@ARTICLE{Dembczynski2012-tv,
  title     = "On label dependence and loss minimization in multi-label
               classification",
  author    = "Dembczy\'{n}ski, Krzysztof and Waegeman, Willem and Cheng,
               Weiwei and H{\"{u}}llermeier, Eyke",
  abstract  = "Abstract Most of the multi-label classification (MLC) methods
               proposed in recent years intended to exploit, in one way or the
               other, dependencies between the class labels. Comparing to
               simple binary relevance learning as a baseline, any gain in
               performance is ...",
  journal   = "Mach. Learn.",
  publisher = "Springer US",
  volume    =  88,
  number    = "1-2",
  pages     = "5--45",
  month     =  "8~" # jun,
  year      =  2012,
  keywords  = "Problem Transformation;Multi Label Classification"
}

@INPROCEEDINGS{Mencia2008-rh,
  title     = "Pairwise learning of multilabel classifications with perceptrons",
  booktitle = "Neural Networks, 2008. {IJCNN} 2008. ({IEEE} World Congress on
               Computational Intelligence). {IEEE} International Joint
               Conference on",
  author    = "Mencia, E Loza and Furnkranz, J",
  abstract  = "Multiclass multilabel perceptrons (MMP) have been proposed as an
               efficient incremental training algorithm for addressing a
               multilabel prediction task with a team of perceptrons. The key
               idea is to train one binary classifier per label, as is
               typically done for addressing multilabel problems, but to make
               the training signal dependent on the performance of the whole
               ensemble. In this paper, we propose an alternative technique
               that is based on a pairwise approach, i.e., we incrementally
               train a perceptron for each pair of classes. Our evaluation on
               four multilabel datasets shows that the multilabel pairwise
               perceptron (MLPP) algorithm yields substantial improvements over
               MMP in terms of ranking quality and overfitting resistance,
               while maintaining its efficiency. Despite the quadratic increase
               in the number of perceptrons that have to be trained, the
               increase in computational complexity is bounded by the average
               number of labels per training example.",
  publisher = "ieeexplore.ieee.org",
  pages     = "2899--2906",
  month     =  jun,
  year      =  2008,
  keywords  = "learning (artificial intelligence);perceptrons;multiclass
               multilabel perceptrons;multilabel classifications;pairwise
               learning;Algorithm design and analysis;Artificial neural
               networks;Classification algorithms;Complexity
               theory;Joints;Prediction
               algorithms;Training;Perceptrons;Pairwise Comparison;Problem
               Transformation;Multi Label Classification"
}

@ARTICLE{Zhang2009-xt,
  title     = "Feature selection for multi-label naive Bayes classification",
  author    = "Zhang, Min-Ling and Pe\~{n}a, Jos\'{e} M and Robles, Victor",
  abstract  = "In multi-label learning, the training set is made up of
               instances each associated with a set of labels, and the task is
               to predict the label sets of unseen instances. In this paper,
               this learning problem is addressed by using a method called Mlnb
               which adapts the traditional naive Bayes classifiers to deal
               with multi-label instances. Feature selection mechanisms are
               incorporated into Mlnb to improve its performance. Firstly,
               feature extraction techniques based on principal component
               analysis are applied to remove irrelevant and redundant
               features. After that, feature subset selection techniques based
               on genetic algorithms are used to choose the most appropriate
               subset of features for prediction. Experiments on synthetic and
               real-world data show that Mlnb achieves comparable performance
               to other well-established multi-label learning algorithms.",
  journal   = "Inf. Sci.",
  publisher = "Elsevier",
  volume    =  179,
  number    =  19,
  pages     = "3218--3229",
  month     =  "9~" # sep,
  year      =  2009,
  keywords  = "Multi-label learning; Naive Bayes; Feature selection; Principal
               component analysis; Genetic algorithm;Multi Label Classification"
}

@ARTICLE{Cesa-Bianchi2006-ll,
  title     = "Incremental Algorithms for Hierarchical Classification",
  author    = "Cesa-Bianchi, Nicol\`{o} and Gentile, Claudio and Zaniboni, Luca",
  abstract  = "Abstract We study the problem of classifying data in a given
               taxonomy when classifications associated with multiple and/or
               partial paths are allowed. We introduce a new algorithm that
               incrementally learns a linear-threshold classifier for each node
               of the taxonomy. A ...",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  7,
  pages     = "31--54",
  month     =  dec,
  year      =  2006,
  keywords  = "Binary Relevance;Problem Transformation;Multi Label
               Classification"
}

@INPROCEEDINGS{Cesa-Bianchi2006-fk,
  title     = "Hierarchical Classification: Combining Bayes with {SVM}",
  booktitle = "Proceedings of the 23rd International Conference on Machine
               Learning",
  author    = "Cesa-Bianchi, Nicol\`{o} and Gentile, Claudio and Zaniboni, Luca",
  abstract  = "Abstract We study hierarchical classification in the general
               case when an instance could belong to more than one class node
               in the underlying taxonomy. Experiments done in previous work
               showed that a simple hierarchy of Support Vectors Machines (SVM)
               with a ...",
  publisher = "ACM",
  pages     = "177--184",
  series    = "ICML '06",
  year      =  2006,
  address   = "New York, NY, USA",
  keywords  = "Binary Relevance;Problem Transformation;Multi Label
               Classification"
}

@INPROCEEDINGS{Tsoumakas2009-ex,
  title     = "Correlation-based pruning of stacked binary relevance models for
               multi-label learning",
  booktitle = "Proceedings of the 1st International Workshop on Learning from
               {Multi-Label} Data",
  author    = "Tsoumakas, Grigorios and Dimou, Anastasios and Spyromitros,
               Eleftherios and Mezaris, Vasileios and Kompatsiaris, Ioannis and
               Vlahavas, Ioannis",
  abstract  = "Abstract. Binary relevance (BR) learns a single binary model for
               each different label of multi - label data. It has linear
               complexity with respect to the number of labels , but does not
               take into account label correlations and may fail to accurately
               predict label combinations and ...",
  publisher = "researchgate.net",
  pages     = "101--116",
  year      =  2009,
  keywords  = "Binary Relevance;Problem Transformation;Multi Label
               Classification"
}

@INCOLLECTION{Tsoumakas2007-wm,
  title     = "Random k-Labelsets: An Ensemble Method for Multilabel
               Classification",
  booktitle = "Machine Learning: {ECML} 2007",
  author    = "Tsoumakas, Grigorios and Vlahavas, Ioannis",
  abstract  = "... To alleviate this problem, we plan as future work to couple
               RAKEL with an ensemble ... Friberg for his valuable contribution
               in the development of the Java software for multilabel
               classification ... 239--240 (2003) 2. Clare, A., King, R.:
               Knowledge discovery in multi - label phenotype data ...",
  publisher = "Springer Berlin Heidelberg",
  pages     = "406--417",
  series    = "Lecture Notes in Computer Science",
  month     =  "17~" # sep,
  year      =  2007,
  keywords  = "Influent;Label Powerset;Problem Transformation;*estudar*;Multi
               Label Classification"
}

@ARTICLE{Read2010-rn,
  title    = "Scalable multi-label classification",
  author   = "Read, Jesse",
  journal  = "Hamilton, New Zealand: University of Waikato",
  year     =  2010,
  keywords = "Multi Label Classification"
}

@ARTICLE{Tai2012-xa,
  title       = "Multilabel classification with principal label space
                 transformation",
  author      = "Tai, Farbound and Lin, Hsuan-Tien",
  affiliation = "Department of Computer Science, National Taiwan University,
                 Taipei 106, Taiwan. b94901176@ntu.edu.tw",
  abstract    = "We consider a hypercube view to perceive the label space of
                 multilabel classification problems geometrically. The view
                 allows us not only to unify many existing multilabel
                 classification approaches but also design a novel algorithm,
                 principal label space transformation (PLST), that captures key
                 correlations between labels before learning. The simple and
                 efficient PLST relies on only singular value decomposition as
                 the key step. We derive the theoretical guarantee of PLST and
                 evaluate its empirical performance using real-world data sets.
                 Experimental results demonstrate that PLST is faster than the
                 traditional binary relevance approach and is superior to the
                 modern compressive sensing approach in terms of both accuracy
                 and efficiency.",
  journal     = "Neural Comput.",
  volume      =  24,
  number      =  9,
  pages       = "2508--2542",
  month       =  sep,
  year        =  2012,
  keywords    = "Label Correlation;Multi Label Classification"
}

@INCOLLECTION{Petterson2010-pf,
  title     = "Reverse {Multi-Label} Learning",
  booktitle = "Advances in Neural Information Processing Systems 23",
  author    = "Petterson, James and Caetano, Tib\'{e}rio S",
  editor    = "Lafferty, J D and Williams, C K I and Shawe-Taylor, J and Zemel,
               R S and Culotta, A",
  publisher = "Curran Associates, Inc.",
  pages     = "1912--1920",
  year      =  2010,
  keywords  = "Multi Label Classification"
}

@ARTICLE{Bielza2011-ia,
  title    = "Multi-dimensional classification with Bayesian networks",
  author   = "Bielza, C and Li, G and Larra\~{n}aga, P",
  abstract = "Multi-dimensional classification aims at finding a function that
              assigns a vector of class values to a given vector of features.
              In this paper, this problem is tackled by a general family of
              models, called multi-dimensional Bayesian network classifiers
              (MBCs). This probabilistic graphical model organizes class and
              feature variables as three different subgraphs: class subgraph,
              feature subgraph, and bridge (from class to features) subgraph.
              Under the standard 0--1 loss function, the most probable
              explanation (MPE) must be computed, for which we provide
              theoretical results in both general MBCs and in MBCs decomposable
              into maximal connected components. Moreover, when computing the
              MPE, the vector of class values is covered by following a special
              ordering (gray code). Under other loss functions defined in
              accordance with a decomposable structure, we derive theoretical
              results on how to minimize the expected loss. Besides these
              inference issues, the paper presents flexible algorithms for
              learning MBC structures from data based on filter, wrapper and
              hybrid approaches. The cardinality of the search space is also
              given. New performance evaluation metrics adapted from the
              single-class setting are introduced. Experimental results with
              three benchmark data sets are encouraging, and they outperform
              state-of-the-art algorithms for multi-label classification.",
  journal  = "Int. J. Approx. Reason.",
  volume   =  52,
  number   =  6,
  pages    = "705--727",
  month    =  sep,
  year     =  2011,
  keywords = "Multi-dimensional outputs; Bayesian network classifiers; Learning
              from data; MPE; Multi-label classification;Multi Label
              Classification"
}

@INPROCEEDINGS{Zhang2010-ee,
  title     = "Multi-label Learning by Exploiting Label Dependency",
  booktitle = "Proceedings of the 16th {ACM} {SIGKDD} International Conference
               on Knowledge Discovery and Data Mining",
  author    = "Zhang, Min-Ling and Zhang, Kun",
  publisher = "ACM",
  pages     = "999--1008",
  series    = "KDD '10",
  year      =  2010,
  address   = "New York, NY, USA",
  keywords  = "bayesian network, machine learning, multi-label
               learning;*estudar*;Multi Label Classification"
}

@INPROCEEDINGS{Read2008-bt,
  title     = "Multi-label Classification Using Ensembles of Pruned Sets",
  booktitle = "Data Mining, 2008. {ICDM} '08. Eighth {IEEE} International
               Conference on",
  author    = "Read, J and Pfahringer, B and Holmes, G",
  abstract  = "This paper presents a pruned sets method (PS) for multi-label
               classification. It is centred on the concept of treating sets of
               labels as single labels. This allows the classification process
               to inherently take into account correlations between labels. By
               pruning these sets, PS focuses only on the most important
               correlations, which reduces complexity and improves accuracy. By
               combining pruned sets in an ensemble scheme (EPS), new label
               sets can be formed to adapt to irregular or complex data. The
               results from experimental evaluation on a variety of multi-label
               datasets show that [E]PS can achieve better performance and
               train much faster than other multi-label methods.",
  publisher = "ieeexplore.ieee.org",
  pages     = "995--1000",
  month     =  dec,
  year      =  2008,
  keywords  = "data mining;pattern classification;ensemble scheme;multilabel
               classification;pruned sets method;Bioinformatics;Computer
               science;Data mining;Genomics;Layout;Nearest neighbor
               searches;Support vector machine classification;Support vector
               machines;Text categorization;Tin;multi-label
               classification;problem transformation;Label Powerset;Problem
               Transformation;Multi Label Classification"
}

@ARTICLE{Boutell2004-gc,
  title     = "Learning multi-label scene classification",
  author    = "Boutell, Matthew R and Luo, Jiebo and Shen, Xipeng and Brown,
               Christopher M",
  abstract  = "In classic pattern recognition problems, classes are mutually
               exclusive by definition. Classification errors occur when the
               classes overlap in the feature space. We examine a different
               situation, occurring when the classes are, by definition, not
               mutually exclusive. Such problems arise in semantic scene and
               document classification and in medical diagnosis. We present a
               framework to handle such problems and apply it to the problem of
               semantic scene classification, where a natural scene may contain
               multiple objects such that the scene can be described by
               multiple class labels (e.g., a field scene with a mountain in
               the background). Such a problem poses challenges to the classic
               pattern recognition paradigm and demands a different treatment.
               We discuss approaches for training and testing in this scenario
               and introduce new metrics for evaluating individual examples,
               class recall and precision, and overall accuracy. Experiments
               show that our methods are suitable for scene classification;
               furthermore, our work appears to generalize to other
               classification problems of the same nature.",
  journal   = "Pattern Recognit.",
  publisher = "Elsevier",
  volume    =  37,
  number    =  9,
  pages     = "1757--1771",
  month     =  sep,
  year      =  2004,
  keywords  = "Image understanding; Semantic scene classification; Multi-label
               classification; Multi-label training; Multi-label evaluation;
               Image organization; Cross-training; Jaccard similarity;Multi
               Label Classification"
}

@ARTICLE{Reyes2016-gy,
  title    = "Effective lazy learning algorithm based on a data gravitation
              model for multi-label learning",
  author   = "Reyes, Oscar and Morell, Carlos and Ventura, Sebasti\'{a}n",
  abstract = "Abstract In the last decade, an increasing number of real-world
              problems surrounding multi-label data have appeared, and
              multi-label learning has become an important area of research.
              The data gravitation model is an approach that applies the
              principles of the universal law of gravitation to resolve machine
              learning problems. One advantage of the data gravitation model,
              compared with other techniques, is that it is based on simple
              principles with high performance levels. This paper presents a
              multi-label lazy algorithm based on a data gravitation model,
              named MLDGC. MLDGC directly handles multi-label data, and
              considers each instance as an atomic data particle. The proposed
              multi-label lazy algorithm was evaluated and compared to several
              state-of-the-art multi-label lazy methods on 34 datasets. The
              results showed that our proposal outperformed state-of-the-art
              lazy methods. The experimental results were validated using
              non-parametric statistical tests, confirming the effectiveness of
              this data gravitation model for multi-label lazy learning.",
  journal  = "Inf. Sci.",
  volume   = "340--341",
  pages    = "159--174",
  month    =  "1~" # may,
  year     =  2016,
  keywords = "Multi-label learning; Data gravitation model; Lazy learning;
              Multi-label classification; Label ranking;Multi Label
              Classification"
}

@ARTICLE{Zhang2015-rs,
  title     = "Lift: {Multi-Label} Learning with {Label-Specific} Features",
  author    = "Zhang, M L and Wu, L",
  abstract  = "Multi-label learning deals with the problem where each example
               is represented by a single instance (feature vector) while
               associated with a set of class labels. Existing approaches learn
               from multi-label data by manipulating with identical feature
               set, i.e. the very instance representation of each example is
               employed in the discrimination processes of all class labels.
               However, this popular strategy might be suboptimal as each label
               is supposed to possess specific characteristics of its own. In
               this paper, another strategy to learn from multi-label data is
               studied, where label-specific features are exploited to benefit
               the discrimination of different class labels. Accordingly, an
               intuitive yet effective algorithm named LIFT, i.e. multi-label
               learning with Label specific Features, is proposed. LIFT firstly
               constructs features specific to each label by conducting
               clustering analysis on its positive and negative instances, and
               then performs training and testing by querying the clustering
               results. Comprehensive experiments on a total of 17 benchmark
               data sets clearly validate the superiority of LIFT against other
               well-established multi-label learning algorithms as well as the
               effectiveness of label-specific features.",
  journal   = "IEEE Trans. Pattern Anal. Mach. Intell.",
  publisher = "ieeexplore.ieee.org",
  volume    =  37,
  number    =  1,
  pages     = "107--120",
  month     =  jan,
  year      =  2015,
  keywords  = "learning (artificial intelligence);pattern
               clustering;LIFT;clustering analysis;multi label learning with
               label specific feature algorithm;Algorithm design and
               analysis;Clustering algorithms;Correlation;Measurement;Text
               categorization;Training;Vectors;Machine learning;label
               correlations;label-specific features;multi-label learning;Multi
               Label Classification"
}

@ARTICLE{Zhang2007-id,
  title     = "{ML-KNN}: A lazy learning approach to multi-label learning",
  author    = "Zhang, Min-Ling and Zhou, Zhi-Hua",
  abstract  = "Multi-label learning originated from the investigation of text
               categorization problem, where each document may belong to
               several predefined topics simultaneously. In multi-label
               learning, the training set is composed of instances each
               associated with a set of labels, and the task is to predict the
               label sets of unseen instances through analyzing training
               instances with known label sets. In this paper, a multi-label
               lazy learning approach named ML-KNN is presented, which is
               derived from the traditional K-nearest neighbor (KNN) algorithm.
               In detail, for each unseen instance, its K nearest neighbors in
               the training set are firstly identified. After that, based on
               statistical information gained from the label sets of these
               neighboring instances, i.e. the number of neighboring instances
               belonging to each possible class, maximum a posteriori (MAP)
               principle is utilized to determine the label set for the unseen
               instance. Experiments on three different real-world multi-label
               learning problems, i.e. Yeast gene functional analysis, natural
               scene classification and automatic web page categorization, show
               that ML-KNN achieves superior performance to some
               well-established multi-label learning algorithms.",
  journal   = "Pattern Recognit.",
  publisher = "Elsevier",
  volume    =  40,
  number    =  7,
  pages     = "2038--2048",
  month     =  jul,
  year      =  2007,
  keywords  = "Machine learning; Multi-label learning; Lazy learning; K-nearest
               neighbor; Functional genomics; Natural scene classification;
               Text categorization;Influent;Multi Label Classification"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Grigorios_Tsoumakas2009-yv,
  title    = "{LEARNING} {FROM} {MULTI-LABEL} {DATA}",
  author   = "Grigorios Tsoumakas, Min-Ling Zhang",
  abstract = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
              Teregowda): This volume contains research papers accepted for
              presentation at the 1st International Workshop on Learning from
              Multi-Label Data (MLDâ€™09), which will be held in Bled, Slovenia,
              at September 7, 2009 in conjunction with ECML/PKDD 2009. MLDâ€™09
              is devoted to multi-label learning, which is an emerging and
              promising research topic of machine learning. In multi-label
              learning, each example is associated with multiple labels
              simultaneously, which therefore encompasses traditional
              supervised learning (single-label) as its special case.
              Multi-label learning is related to various machine learning
              paradigms, such as classification, ranking, semi-supervised
              learning, active learning, multi-instance learning,
              dimensionality reduction, etc. Initial attempts on multi-label
              learning date back to 1999 with works on multi-label text
              categorization. In recent years, the task of learning from
              multi-label data has been addressed by a number of methods
              adapted from various popular learning techniques, such as neural
              networks, decision trees, k-nearest neighbors, kernel methods,
              ensemble methods, etc. More impressively, multi-label learning
              has manifested its effectiveness in a diversity of real-world
              applications, such as image/video annotation, bioinformatics,",
  year     =  2009,
  keywords = "Multi Label Classification"
}

@INCOLLECTION{De_Carvalho2009-yp,
  title     = "A Tutorial on Multi-label Classification Techniques",
  booktitle = "Foundations of Computational Intelligence Volume 5",
  author    = "de Carvalho, Andr\'{e} C P L F and Freitas, Alex A",
  publisher = "Springer Berlin Heidelberg",
  pages     = "177--195",
  series    = "Studies in Computational Intelligence",
  year      =  2009,
  keywords  = "Overview;Multi Label Classification"
}

@ARTICLE{Zhang2014-be,
  title    = "A Review on {Multi-Label} Learning Algorithms",
  author   = "Zhang, Min-Ling and Zhou, Zhi-Hua",
  abstract = "Multi-label learning studies the problem where each example is
              represented by a single instance while associated with a set of
              labels simultaneously. During the past decade, significant amount
              of progresses have been made toward this emerging machine
              learning paradigm. This paper aims to provide a timely review on
              this area with emphasis on state-of-the-art multi-label learning
              algorithms. Firstly, fundamentals on multi-label learning
              including formal definition and evaluation metrics are given.
              Secondly and primarily, eight representative multi-label learning
              algorithms are scrutinized under common notations with relevant
              analyses and discussions. Thirdly, several related learning
              settings are briefly summarized. As a conclusion, online
              resources and open research problems on multi-label learning are
              outlined for reference purposes.",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  volume   =  26,
  number   =  8,
  pages    = "1819--1837",
  month    =  aug,
  year     =  2014,
  keywords = "learning (artificial intelligence);evaluation metrics;formal
              definition;instance learning;learning settings;machine learning
              paradigm;multilabel learning algorithms;Algorithm design and
              analysis;Correlation;Machine learning
              algorithms;Semantics;Supervised
              learning;Training;Vectors;Artificial Intelligence;Computing
              Methodologies;Data mining;Database Applications;Database
              Management;Information Technology and
              Systems;Learning;Multi-label learning;algorithm adaptation;label
              correlations;problem transformation;Overview;Multi Label
              Classification"
}

@MISC{noauthor_undated-sp,
  title        = "Mulan: A Java library for multi-label learning",
  howpublished = "\url{http://mulan.sourceforge.net/}",
  note         = "Accessed: 2016-2-14",
  keywords     = "Multi Label Classification"
}

@INCOLLECTION{Clare2001-tq,
  title     = "Knowledge Discovery in Multi-label Phenotype Data",
  booktitle = "Principles of Data Mining and Knowledge Discovery",
  author    = "Clare, Amanda and King, Ross D",
  publisher = "Springer Berlin Heidelberg",
  pages     = "42--53",
  series    = "Lecture Notes in Computer Science",
  month     =  "3~" # sep,
  year      =  2001,
  keywords  = "Algorithm Adaptation;Multi Label Classification"
}

@PHDTHESIS{Cerri2010-qr,
  title     = "T\'{e}cnicas de classifica\c{c}\~{a}o hier\'{a}rquica
               multirr\'{o}tulo",
  author    = "Cerri, Ricardo",
  abstract  = "Muitos dos problemas de classifica\c{c}\~{a}o descritos na
               literatura de Aprendizado de M\'{a}quina e Minera\c{c}\~{a}o de
               Dados dizem respeito \`{a} classifica\c{c}\~{a}o de dados em que
               cada exemplo a ser classificado pertence a um conjunto finito, e
               geralmente pequeno, de classes que est\~{a}o ...",
  publisher = "teses.usp.br",
  year      =  2010,
  school    = "Universidade de S\~{a}o Paulo",
  keywords  = "Problem Transformation;Multi Label Classification"
}

@PHDTHESIS{Vallim2009-kd,
  title    = "Sistemas classificadores evolutivos para problemas
              multirr\'{o}tulo",
  author   = "Vallim, Rosane Maria Maffei",
  year     =  2009,
  school   = "Universidade de S\~{a}o Paulo",
  keywords = "Multi Label Classification"
}

@INPROCEEDINGS{Cherman2009-ht,
  title     = "Um estudo sobre m\'{e}todos de classifica\c{c}\~{a}o
               multirr\'{o}tulo",
  booktitle = "Anais do {IV} Congresso da Academia Trinacional de Ci\^{e}ncias",
  author    = "Cherman, E A and Monard, M C",
  pages     = "1--10",
  year      =  2009,
  keywords  = "Multi Label Classification"
}

@INPROCEEDINGS{Cherman2010-tw,
  title     = "M\'{e}todos multirr\'{o}tulo independentes de algoritmo: um
               estudo de caso",
  booktitle = "Proceedings of the {XXXVI} Conferencia Latinoamericana de
               Inform\'{a}tica ({CLEI})",
  author    = "Cherman, Everton Alvares and Metz, Jean and Monard, Maria
               Carolina",
  pages     = "1--14",
  year      =  2010,
  keywords  = "Multi Label Classification"
}

@ARTICLE{Katakis2008-hz,
  title    = "Multilabel text classification for automated tag suggestion",
  author   = "Katakis, Ioannis and Tsoumakas, Grigorios and Vlahavas, Ioannis",
  journal  = "ECML PKDD discovery challenge",
  volume   =  75,
  year     =  2008,
  keywords = "Multi Label Classification"
}

@INPROCEEDINGS{Cheng2010-pj,
  title     = "Bayes optimal multilabel classification via probabilistic
               classifier chains",
  booktitle = "Proceedings of the 27th international conference on machine
               learning ({ICML-10})",
  author    = "Cheng, Weiwei and H{\"{u}}llermeier, Eyke and Dembczynski,
               Krzysztof J",
  pages     = "279--286",
  year      =  2010,
  keywords  = "Multi Label Classification"
}

@ARTICLE{Madjarov2012-on,
  title    = "An extensive experimental comparison of methods for multi-label
              learning",
  author   = "Madjarov, Gjorgji and Kocev, Dragi and Gjorgjevikj, Dejan and
              D\v{z}eroski, Sa\v{s}o",
  abstract = "Multi-label learning has received significant attention in the
              research community over the past few years: this has resulted in
              the development of a variety of multi-label learning methods. In
              this paper, we present an extensive experimental comparison of 12
              multi-label learning methods using 16 evaluation measures over 11
              benchmark datasets. We selected the competing methods based on
              their previous usage by the community, the representation of
              different groups of methods and the variety of basic underlying
              machine learning methods. Similarly, we selected the evaluation
              measures to be able to assess the behavior of the methods from a
              variety of view-points. In order to make conclusions independent
              from the application domain, we use 11 datasets from different
              domains. Furthermore, we compare the methods by their efficiency
              in terms of time needed to learn a classifier and time needed to
              produce a prediction for an unseen example. We analyze the
              results from the experiments using Friedman and Nemenyi tests for
              assessing the statistical significance of differences in
              performance. The results of the analysis show that for
              multi-label classification the best performing methods overall
              are random forests of predictive clustering trees (RF-PCT) and
              hierarchy of multi-label classifiers (HOMER), followed by binary
              relevance (BR) and classifier chains (CC). Furthermore, RF-PCT
              exhibited the best performance according to all measures for
              multi-label ranking. The recommendation from this study is that
              when new methods for multi-label learning are proposed, they
              should be compared to RF-PCT and HOMER using multiple evaluation
              measures.",
  journal  = "Pattern Recognit.",
  volume   =  45,
  number   =  9,
  pages    = "3084--3104",
  month    =  sep,
  year     =  2012,
  keywords = "Multi-label ranking; Multi-label classification; Comparison of
              multi-label learning methods;Multi Label Classification"
}

@INPROCEEDINGS{Ghamrawi2005-fw,
  title     = "Collective Multi-label Classification",
  booktitle = "Proceedings of the 14th {ACM} International Conference on
               Information and Knowledge Management",
  author    = "Ghamrawi, Nadia and McCallum, Andrew",
  publisher = "ACM",
  pages     = "195--200",
  series    = "CIKM '05",
  year      =  2005,
  address   = "New York, NY, USA",
  keywords  = "classification, machine learning, multi-label, statistical
               learning, uncertainty;Algorithm Adaptation;Multi Label
               Classification"
}

@ARTICLE{Zhou2008-lf,
  title         = "{Multi-Instance} {Multi-Label} Learning",
  author        = "Zhou, Zhi-Hua and Zhang, Min-Ling and Huang, Sheng-Jun and
                   Li, Yu-Feng",
  abstract      = "In this paper, we propose the MIML (Multi-Instance
                   Multi-Label learning) framework where an example is
                   described by multiple instances and associated with multiple
                   class labels. Compared to traditional learning frameworks,
                   the MIML framework is more convenient and natural for
                   representing complicated objects which have multiple
                   semantic meanings. To learn from MIML examples, we propose
                   the MimlBoost and MimlSvm algorithms based on a simple
                   degeneration strategy, and experiments show that solving
                   problems involving complicated objects with multiple
                   semantic meanings in the MIML framework can lead to good
                   performance. Considering that the degeneration process may
                   lose information, we propose the D-MimlSvm algorithm which
                   tackles MIML problems directly in a regularization
                   framework. Moreover, we show that even when we do not have
                   access to the real objects and thus cannot capture more
                   information from real objects by using the MIML
                   representation, MIML is still useful. We propose the InsDif
                   and SubCod algorithms. InsDif works by transforming
                   single-instances into the MIML representation for learning,
                   while SubCod works by transforming single-label examples
                   into the MIML representation for learning. Experiments show
                   that in some tasks they are able to achieve better
                   performance than learning the single-instances or
                   single-label examples directly.",
  month         =  "24~" # aug,
  year          =  2008,
  keywords      = "Multi Label Classification",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "0808.3231"
}

@ARTICLE{Vens2008-yp,
  title     = "Decision trees for hierarchical multi-label classification",
  author    = "Vens, Celine and Struyf, Jan and Schietgat, Leander and
               D\v{z}eroski, Sa\v{s}o and Blockeel, Hendrik",
  journal   = "Mach. Learn.",
  publisher = "Springer US",
  volume    =  73,
  number    =  2,
  pages     = "185--214",
  month     =  "1~" # aug,
  year      =  2008,
  keywords  = "Multi Label Classification"
}

@INPROCEEDINGS{Elisseeff2001-lp,
  title     = "A kernel method for multi-labelled classification",
  booktitle = "Advances in neural information processing systems",
  author    = "Elisseeff, Andr\'{e} and Weston, Jason",
  abstract  = "This article presents a Support Vector Machine (SVM) like
               learning system to handle multi-label problems. Such problems
               are usually decomposed into many two-class problems but the
               expressive power of such a system can be weak. We explore a new
               direct approach. It is based on a large margin ranking system
               that shares a lot of common properties with SVMs. We tested it
               on a Yeast gene functional classification problem with positive
               results.",
  pages     = "681--687",
  year      =  2001,
  keywords  = "Multi Label Classification"
}

@INCOLLECTION{Tsoumakas2009-vw,
  title     = "Mining Multi-label Data",
  booktitle = "Data Mining and Knowledge Discovery Handbook",
  author    = "Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis",
  publisher = "Springer US",
  pages     = "667--685",
  year      =  2009,
  keywords  = "Overview;Multi Label Classification"
}

@ARTICLE{Read2011-es,
  title     = "Classifier chains for multi-label classification",
  author    = "Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and
               Frank, Eibe",
  journal   = "Mach. Learn.",
  publisher = "Springer US",
  volume    =  85,
  number    =  3,
  pages     = "333--359",
  month     =  "30~" # jun,
  year      =  2011,
  keywords  = "Binary Relevance;Problem Transformation;Multi Label
               Classification"
}

@ARTICLE{Tsoumakas2007-cw,
  title    = "Multi-label classification: An overview",
  author   = "Tsoumakas, Grigorios and Katakis, Ioannis",
  journal  = "International Journal of Data Warehousing and Mining",
  number   =  3,
  pages    = "1--13",
  year     =  2007,
  keywords = "Overview;Multi Label Classification"
}

